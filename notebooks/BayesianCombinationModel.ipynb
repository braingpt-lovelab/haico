{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Combination Model\n",
    "\n",
    "Example notebook for the BrainGPT project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import submitit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from haico.Bayesian_HM_model import BayesianCombinationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(logsdir=\"logs\"):\n",
    "    # get the hostname\n",
    "    hostname = os.uname().nodename\n",
    "    \n",
    "    if 'SUBMITIT_EXECUTOR' in os.environ:\n",
    "        logger = logging.getLogger(\"submitit\") # using submitit task logger\n",
    "        print(f'using submitit logger at {hostname}')\n",
    "    else :\n",
    "        # using hostname as the logger name\n",
    "        logger = logging.getLogger(hostname)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # avoid duplicated handlers (duplicated log messages)\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    if not os.path.exists(logsdir):\n",
    "        os.makedirs(logsdir)\n",
    "\n",
    "    # today date\n",
    "    import datetime\n",
    "\n",
    "    today = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    logfile = os.path.join(logsdir, \"output_{}.log\".format(today))\n",
    "    formatter = logging.Formatter(\"{levelname} [{name}]: {asctime} - {message}\", style=\"{\")\n",
    "\n",
    "    # Console handler\n",
    "    chandler = logging.StreamHandler()\n",
    "    chandler.setLevel(logging.DEBUG)\n",
    "    chandler.setFormatter(formatter)\n",
    "    logger.addHandler(chandler)\n",
    "\n",
    "    # File handler\n",
    "    fhandler = logging.FileHandler(logfile, \"a\")\n",
    "    fhandler.setLevel(logging.DEBUG)\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.info(f\"Logging to: {logfile}\")\n",
    "\n",
    "    return logger\n",
    "\n",
    "# get the parent directory of the current directory\n",
    "parentdir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# using logger to log messages on the console and in a file\n",
    "logsdir = os.path.join(parentdir, \"logs\")\n",
    "logger = setup_logging(logsdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for PyTorch\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# If using CUDA, set the random seed for CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# Set parameters for the Bayesian model\n",
    "num_samples, warmup_steps, num_chains = 25, 500, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare human-machine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the data directory\n",
    "root_path = '../data/'\n",
    "\n",
    "# List of classifiers to be analyzed\n",
    "selected_LLMs = ['meta-llama--Llama-2-7b-chat-hf', \n",
    "                 'meta-llama--Llama-2-13b-chat-hf', \n",
    "                 'meta-llama--Llama-2-70b-chat-hf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read human participants data\n",
    "online_study = pd.read_csv(f\"{root_path}human/data/participant_data.csv\")\n",
    "\n",
    "# Select GPT-4 generated abstracts\n",
    "abstract_idx = online_study['journal_section'].str.startswith('machine')\n",
    "online_study = online_study[abstract_idx]\n",
    "\n",
    "# Extract DOI links from all test cases assessed by human participants\n",
    "doi = pd.read_csv(f\"{root_path}human/abstract_id_doi.csv\")\n",
    "doi = doi['DOI;abstract_id;abstract'].str.split(';', expand=True)[[0,1]]\n",
    "doi.columns = ['doi', 'abstract_id']\n",
    "\n",
    "# Extract DOI links from GPT-4 generated test cases\n",
    "gpt4_doi = pd.read_csv(f\"{root_path}testcases/BrainBench_GPT-4_v0.1.csv\")\n",
    "\n",
    "# Reorder human participants data based on the order of GPT-4 generated abstracts\n",
    "gpt4_order = gpt4_doi.merge(doi, on='doi')['abstract_id'].astype(float)\n",
    "online_study['abstract_id'] = pd.Categorical(online_study['abstract_id'], categories=gpt4_order, ordered=True)\n",
    "online_study = online_study.sort_values('abstract_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classification and confidence dataframes\n",
    "classification = pd.DataFrame()\n",
    "classification.loc[:,'abstract_id'] = np.array([np.where(gpt4_order==i)[0][0] for i in online_study['abstract_id']])\n",
    "confidence = classification.copy()\n",
    "\n",
    "# Set ground truth labels\n",
    "order_labels = np.load(f\"{root_path}machine/model_results/{selected_LLMs[0]}/llm_abstracts/labels.npy\")\n",
    "classification = classification.merge(pd.DataFrame(order_labels, columns=['true labels']), left_on='abstract_id', right_index=True)\n",
    "\n",
    "# Set human classification and confidence\n",
    "classification.loc[:,'Human'] = np.array([j if i == 1 else 1 - j for i, j in zip(online_study['correct'], classification['true labels'])])\n",
    "confidence.loc[:,'Human'] = np.where(online_study['confidence'].values > 66, 2, np.where(online_study['confidence'].values <= 33, 0, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=None):\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in selected_LLMs:\n",
    "    \n",
    "    # Read PPL scores of machine classifiers\n",
    "    machine_PPL = np.load(f\"{root_path}machine/model_results/{i}/llm_abstracts/PPL_A_and_B.npy\")\n",
    "    \n",
    "    # Get classification results\n",
    "    machine_name = i.lstrip('meta-llama--Llama-2-').rstrip('-chat-hf').upper()\n",
    "    machine_classification = pd.DataFrame(np.argmin(machine_PPL, axis=1), columns=[machine_name])\n",
    "    classification = classification.merge(machine_classification, left_on='abstract_id', right_index=True)\n",
    "\n",
    "    # Define confidence as PPL difference\n",
    "    machine_confidence = pd.DataFrame(softmax(machine_PPL, axis=1), columns=[machine_name+'-A',machine_name+'-B'])\n",
    "    confidence = confidence.merge(machine_confidence, left_on='abstract_id', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Bayesian combination model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define process cross_validation_fold as a function\n",
    "def process_cross_validation_fold(fold, machine_clf, N):\n",
    "    \n",
    "    # Train data for current fold\n",
    "    machine_probscores_train = torch.from_numpy(confidence[confidence['abstract_id']!=fold][[machine_clf+'-A',machine_clf+'-B']].values)\n",
    "    human_classification_train = torch.from_numpy(classification[classification['abstract_id']!=fold]['Human'].values).to(torch.int64)\n",
    "    human_confidence_train = torch.from_numpy(confidence[confidence['abstract_id']!=fold]['Human'].values)\n",
    "    truelabel_train = torch.from_numpy(classification[classification['abstract_id']!=fold]['true labels'].values).to(torch.int64)\n",
    "    \n",
    "    # Test data for current fold\n",
    "    machine_probscores_test = torch.from_numpy(confidence[confidence['abstract_id']==fold][[machine_clf+'-A',machine_clf+'-B']].values)\n",
    "    human_classification_test = torch.from_numpy(classification[classification['abstract_id']==fold]['Human'].values).to(torch.int64)\n",
    "    human_confidence_test = torch.from_numpy(confidence[confidence['abstract_id']==fold]['Human'].values)\n",
    "    classification_test = classification[classification['abstract_id']==fold]\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BayesianCombinationModel()\n",
    "    \n",
    "    # This is the training phase (labels are observed)\n",
    "    logger.info(f\"Fold {str(fold+1).rjust(3, ' ')}/{N} | Training...\")\n",
    "    train_trace = model.infer(machine_probscores_train,\n",
    "                              human_classification_train,\n",
    "                              human_confidence_train,\n",
    "                              truelabel=truelabel_train,\n",
    "                              num_samples=num_samples,\n",
    "                              warmup_steps=warmup_steps,\n",
    "                              num_chains=num_chains,\n",
    "                              disable_progbar=True,\n",
    "                              group_by_chain=False)\n",
    "\n",
    "    # Get model parameters for prediction\n",
    "    params = model.params\n",
    "    for key in params.keys():\n",
    "        try:\n",
    "            # Get the mean of the posterior samples, sort is for the case of cutpoints\n",
    "            trace_now = train_trace[key].view(num_chains*num_samples,-1)\n",
    "            trace_now = torch.sort(trace_now, dim=-1)[0].detach()\n",
    "            params[key] = torch.mean(trace_now, dim=0)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # This is the testing phase (labels are latent)\n",
    "    logger.info(f\"Fold {str(fold+1).rjust(3, ' ')}/{N} | Testing...\")\n",
    "    test_trace = model.infer(machine_probscores_test,\n",
    "                             human_classification_test,\n",
    "                             human_confidence_test,\n",
    "                             truelabel=None, params=params,\n",
    "                             num_samples=num_samples,\n",
    "                             warmup_steps=warmup_steps, \n",
    "                             num_chains=num_chains,\n",
    "                             disable_progbar=True,\n",
    "                             group_by_chain=False)\n",
    "    \n",
    "    # Get predictions for this fold\n",
    "    A_pred = classification_test[machine_clf].values == classification_test['true labels'].values\n",
    "    B_pred = classification_test['Human'].values == classification_test['true labels'].values\n",
    "    AB_all = torch.argmax(test_trace['labelprob'], dim=-1)\n",
    "    AB_pred = torch.mode(AB_all, dim=0)[0].numpy() == classification_test['true labels'].values\n",
    "    \n",
    "    # Display predictions\n",
    "    logger.info(f\"Fold {str(j+1).rjust(3, ' ')}/{N} | A: {A_pred.mean():.3f} | B: {B_pred.mean():.3f} | AB: {AB_pred.mean():.3f}\")\n",
    "\n",
    "    # Save prediction accuracy\n",
    "    predictions = pd.DataFrame({'A': A_pred, 'B': B_pred, 'AB': AB_pred})\n",
    "    \n",
    "    # Return the results\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(gpt4_doi)\n",
    "\n",
    "logger.debug(f\"Number of abstracts: {N}\")\n",
    "logger.debug(f\"selected_LLMs: {selected_LLMs}\")\n",
    "\n",
    "logs_submitit = os.path.join(logsdir, 'submitit')\n",
    "logger.info(f'Logs dir submitit: {logs_submitit}')\n",
    "executor = submitit.SlurmExecutor(folder=logs_submitit)\n",
    "\n",
    "executor.update_parameters(\n",
    "    partition=\"CPU,GPU\",\n",
    "    time=\"6:00:00\", # 4 hours\n",
    "    mem=\"350G\",\n",
    "    cpus_per_task=48,\n",
    "    comment=\"haico-job\",\n",
    "    job_name=\"haico-job\",\n",
    "    array_parallelism=100,\n",
    ")\n",
    "\n",
    "for i in range(len(selected_LLMs)):\n",
    "    \n",
    "    machine_clf = classification.columns[3+i]\n",
    "    logger.info(f\"Classifier A: {machine_clf} --- Classifier B: Human\")\n",
    "\n",
    "    # Create a list to store the jobs\n",
    "    jobs = []\n",
    "\n",
    "    # Using array jobs with 100 parallel jobs\n",
    "    with executor.batch():\n",
    "        # Submit the job for each fold in parallel\n",
    "        for j in range(N):\n",
    "            job = executor.submit(process_cross_validation_fold, j, machine_clf, N)\n",
    "            jobs.append(job)\n",
    "\n",
    "    # Wait for all jobs to complete\n",
    "    results = [job.result() for job in jobs]\n",
    "\n",
    "    # Read the results into lists\n",
    "    predictions_list = [result for result in results]\n",
    "\n",
    "    # Convert lists to pandas dataframes\n",
    "    predictions = pd.concat(predictions_list)\n",
    "\n",
    "    # Save predictions for current model\n",
    "    predictions.to_csv(f\"../results/Bayesian_HM_predictions_{machine_clf}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
