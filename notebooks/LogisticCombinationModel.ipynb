{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Combination Model\n",
    "\n",
    "Example notebook for the BrainGPT project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare human-machine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data directory\n",
    "root_path = '../data/'\n",
    "\n",
    "abstracts_fpath = 'testcases/BrainBench_GPT-4_v0.1.csv'\n",
    "\n",
    "# List of classifiers to be analyzed\n",
    "selected_LLMs = ['meta-llama--Llama-2-7b-chat-hf', 'meta-llama--Llama-2-13b-chat-hf', 'meta-llama--Llama-2-70b-chat-hf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read human participants data\n",
    "online_study = pd.read_csv(f\"{root_path}human/data/participant_data.csv\")\n",
    "\n",
    "# Select GPT-4 generated abstracts\n",
    "abstract_idx = online_study['journal_section'].str.startswith('machine')\n",
    "online_study = online_study[abstract_idx]\n",
    "\n",
    "# Extract DOI links from all test cases assessed by human participants\n",
    "doi = pd.read_csv(f\"{root_path}human/abstract_id_doi.csv\")\n",
    "doi = doi['DOI;abstract_id;abstract'].str.split(';', expand=True)[[0,1]]\n",
    "doi.columns = ['doi', 'abstract_id']\n",
    "\n",
    "# Extract DOI links from GPT-4 generated test cases\n",
    "gpt4_doi = pd.read_csv(f\"{root_path}testcases/BrainBench_GPT-4_v0.1.csv\")\n",
    "\n",
    "# Reorder human participants data based on the order of GPT-4 generated abstracts\n",
    "gpt4_order = gpt4_doi.merge(doi, on='doi')['abstract_id'].astype(float)\n",
    "online_study['abstract_id'] = pd.Categorical(online_study['abstract_id'], categories=gpt4_order, ordered=True)\n",
    "online_study = online_study.sort_values('abstract_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classification and confidence dataframes\n",
    "classification = pd.DataFrame()\n",
    "classification.loc[:,'abstract_id'] = np.array([np.where(gpt4_order==i)[0][0] for i in online_study['abstract_id']])\n",
    "confidence = classification.copy()\n",
    "\n",
    "# Set ground truth labels\n",
    "order_labels = np.load(f\"{root_path}machine/model_results/{selected_LLMs[0]}/llm_abstracts/labels.npy\")\n",
    "classification = classification.merge(pd.DataFrame(order_labels, columns=['true labels']), left_on='abstract_id', right_index=True)\n",
    "\n",
    "# Set human classification and confidence\n",
    "classification.loc[:,'Human'] = np.array([j if i == 1 else 1 - j for i, j in zip(online_study['correct'], classification['true labels'])])\n",
    "confidence.loc[:,'Human'] = online_study['confidence'].values * (classification['Human'].values-0.5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in selected_LLMs:\n",
    "    \n",
    "    # Read PPL scores of machine classifiers\n",
    "    machine_PPL = np.load(f\"{root_path}machine/model_results/{i}/llm_abstracts/PPL_A_and_B.npy\")\n",
    "    \n",
    "    # Get classification results\n",
    "    machine_name = i.lstrip('meta-llama--Llama-2-').rstrip('-chat-hf').upper()\n",
    "    machine_classification = pd.DataFrame(np.argmin(machine_PPL, axis=1), columns=[machine_name])\n",
    "    classification = classification.merge(machine_classification, left_on='abstract_id', right_index=True)\n",
    "    \n",
    "    # Define confidence as PPL difference\n",
    "    machine_confidence = pd.DataFrame(pd.DataFrame(machine_PPL).diff(axis=1).iloc[:,1].values, columns=[machine_name])\n",
    "    confidence = confidence.merge(machine_confidence, left_on='abstract_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all possible combinations of classifiers\n",
    "all_classifiers = confidence.columns[1:].values\n",
    "all_combinations = []\n",
    "for i in range(1, len(all_classifiers)+1):\n",
    "    els = [list(x) for x in combinations(all_classifiers, i)]\n",
    "    all_combinations.extend(els)\n",
    "    \n",
    "# Create a dataframe to store predictions\n",
    "predictions = pd.DataFrame(columns=all_classifiers)\n",
    "for i in range(len(all_combinations)):\n",
    "    for element in all_classifiers:\n",
    "        predictions.loc[i, element] = element in all_combinations[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run logistic combination model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Human</th>\n",
       "      <th>7B</th>\n",
       "      <th>13B</th>\n",
       "      <th>70B</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.697813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.725646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.725646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.745527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.785288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.803181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.795229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.765408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.741551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.755467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.801193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.785288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.799205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.727634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.793241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Human     7B    13B    70B  Accuracy\n",
       "0    True  False  False  False  0.697813\n",
       "1   False   True  False  False  0.725646\n",
       "2   False  False   True  False  0.725646\n",
       "3   False  False  False   True  0.745527\n",
       "4    True   True  False  False  0.785288\n",
       "5    True  False   True  False  0.803181\n",
       "6    True  False  False   True  0.795229\n",
       "7   False   True   True  False  0.765408\n",
       "8   False   True  False   True  0.741551\n",
       "9   False  False   True   True  0.755467\n",
       "10   True   True   True  False  0.801193\n",
       "11   True   True  False   True  0.785288\n",
       "12   True  False   True   True  0.799205\n",
       "13  False   True   True   True  0.727634\n",
       "14   True   True   True   True  0.793241"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leave-one-out cross-validation\n",
    "for i in range(len(all_combinations)):\n",
    "    \n",
    "    tmp_pred = pd.DataFrame()\n",
    "    \n",
    "    for j in range(len(machine_PPL)):\n",
    "        \n",
    "        # Train/test data for current fold\n",
    "        X_train = confidence[confidence['abstract_id']!=j]\n",
    "        y_train = classification[classification['abstract_id']!=j]['true labels'].values\n",
    "        \n",
    "        X_test = confidence[confidence['abstract_id']==j]\n",
    "        y_test = classification[classification['abstract_id']==j]\n",
    "    \n",
    "        if i < len(all_classifiers):\n",
    "            \n",
    "            # Get prediction accuracy of single classifiers\n",
    "            acc = y_test[all_classifiers[i]].values == y_test['true labels'].values\n",
    "            \n",
    "            # Save prediction accuracy \n",
    "            tmp_pred = pd.concat([tmp_pred, pd.DataFrame({'Accuracy': acc})])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Train logistic regression model\n",
    "            clf = LogisticRegression(random_state=1).fit(X_train[all_combinations[i]], y_train)\n",
    "            \n",
    "            # Get prediction accuracy for different teams\n",
    "            acc = clf.predict(X_test[all_combinations[i]]) == y_test['true labels'].values\n",
    "            \n",
    "            # Save prediction accuracy \n",
    "            tmp_pred = pd.concat([tmp_pred, pd.DataFrame({'Accuracy': acc})])\n",
    "    \n",
    "    predictions.loc[i, 'Accuracy'] = tmp_pred.mean().values\n",
    "           \n",
    "# Save predictions \n",
    "predictions.to_csv(f\"../results/Logistic_predictions.csv\", index=False)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
